{{ template "chart.header" . }}
{{ template "chart.description" . }}

{{ template "chart.versionBadge" . }}{{ template "chart.typeBadge" . }}{{ template "chart.appVersionBadge" . }}

{{ template "chart.requirementsSection" . }}

## Implementation Details

### OCI availibility
This chart is available both as Helm Repository as well as OCI artefact.
```sh
helm repo add unique https://unique-ag.github.io/helm-charts/
helm install my-{{ template "chart.name" . }} unique/{{ template "chart.name" . }} --version {{ template "chart.version" . }}

# or
helm install my-{{ template "chart.name" . }} oci://ghcr.io/unique-ag/helm-charts/{{ template "chart.name" . }} --version {{ template "chart.version" . }}
```

### Docker Images
The chart itself uses `ghcr.io/unique-ag/chart-testing-service` as its base image. This is due to automation aspects and because there is no specific `appVersion` or service delivered with it. Using `chart-testing-service` Unique can improve the charts quality without dealing with the complexity of private registries during testing. Naturally, when deploying the Unique product, the image will be replaced with the actual Unique image(s). You can inspect the image [here](https://github.com/Unique-AG/helm-charts/tree/main/docker) and it is served from [`ghcr.io/unique-ag/chart-testing-service`](https://github.com/Unique-AG/helm-charts/pkgs/container/chart-testing-service).

### Networking
The chart supports the Gateway API and its resources. This is the recommended way to go forward. The Gateway API is a Kubernetes-native way to manage your networking resources and is part of the [Gateway API](https://gateway-api.sigs.k8s.io) project.
    + The Gateway API is not enabled by default yet, you need to selectively `enable` different `routes` or use `extraRoutes` to configure them. See [Routes](#routes) for more information.

### Ports
<small>added in `3.1.0`</small>

Unique services communicate with each other extensively. Grown organically from local development, each service used to have its own port. This has grown into kubernetes over time. While technically not a problem, it is a nightmare to set up and manage for Application teams. Since `3.1.0` the chart supports a `ports` object that allows to specify ports for the service and deployment/application.

```yaml
ports:
  application: 8080
  service: 80
```
The port is called `application` because it is the port that the Unique application (that gets deployed using this chart) listens on. The object is nested to allow for future expansion in case another port for a container would be needed. It is not called `containerPort` because there are or can be more than one port for a container and more than one container in a pod.

This change allows Application teams to now use Kubernetes internal networking to communicate with each other way more easily without researching and guessing the correct port.
```yaml
# before
ANOTHER_SERVICE_URL: http://backend-service.namespace.svc:8080

# after
ANOTHER_SERVICE_URL: http://backend-service.namespace.svc
```

The new object `ports` allows also to specify a service port for consistency, but a `service.port` will take precedence over the `ports.service` value.
 to keep non-breaking backward compatibility.

#### Routes
With `2.0.0` the chart supports _routes_ (namely all routes from [`gateway.networking.k8s.io`](https://gateway-api.sigs.k8s.io/concepts/api-overview/#route-resources)). While `routes`, not yet enabled by default, abstract certain complexity away from the chart consumer, `extraRoutes` can be used by power-users to configure each route exactly to their needs.

To use _Routes_ per se you need two things:

- Knowledge about them and the Gateway API, you can start from these resources:
    + [Gateway API](https://gateway-api.sigs.k8s.io)
    + [Route Resources](https://gateway-api.sigs.k8s.io/concepts/api-overview/#route-resources)
- CRDs installed
    + [Install CRDs](https://gateway-api.sigs.k8s.io/guides/#getting-started-with-gateway-api)

##### HTTPS Redirects
Routes do not redirect to HTTPS by default. This redirect must be handled in the upstream services or extended via `extraAnnotations`. This is not due to a lack of security but the fact that the chart is agnostic to the upstream service and its configuration so enforcing a redirect can lead to unexpected behavior and indefinite redirects. Also, most modern browsers prefix all requests with `https` as a secure practice.

### Network Policies
<small>added in `4.4.0`</small>

The chart supports Kubernetes Network Policies to control traffic flow to and from pods. Network Policies are a Kubernetes-native way to implement network segmentation and can help improve security by restricting network communication.

#### Network Policy Flavors
<small>added in `4.4.0`</small>

The chart supports two network policy flavors:

- **`kubernetes`** (default): Standard Kubernetes NetworkPolicy resources
- **`cilium`**: Cilium's CiliumNetworkPolicy resources with enhanced features

```yaml
networkPolicy:
  enabled: true
  flavor: kubernetes  # or "cilium"
  policyTypes:
    - Ingress
    - Egress
  ingress:
    # Allow ingress from monitoring namespace
    - from:
        - namespaceSelector:
            matchLabels:
              name: monitoring
      ports:
        - protocol: TCP
          port: 8080
  egress:
    # Allow egress to database namespace
    - to:
        - namespaceSelector:
            matchLabels:
              name: database
      ports:
        - protocol: TCP
          port: 5432
    # Allow external HTTPS/HTTP access
    - to: []
      ports:
        - protocol: TCP
          port: 443
        - protocol: TCP
          port: 80
```

#### Kubernetes vs Cilium Differences

When using `flavor: cilium`, the chart will:
- Use `apiVersion: cilium.io/v2` and `kind: CiliumNetworkPolicy`
- Use `endpointSelector` instead of `podSelector` in the spec
- Support advanced Cilium-specific features in your rules

When using `flavor: kubernetes` (default), the chart will:
- Use `apiVersion: networking.k8s.io/v1` and `kind: NetworkPolicy`
- Use standard `podSelector` in the spec
- Maintain compatibility with any Kubernetes CNI that supports NetworkPolicy

**Important Notes:**
- Network Policies require a CNI (Container Network Interface) that supports them, such as Calico, Cilium, or Weave Net
- For Cilium flavor, you must have Cilium CNI installed with CiliumNetworkPolicy CRDs
- Once a Network Policy is applied to a pod, it becomes "isolated" and only allowed traffic will be permitted
- Always include DNS resolution (port 53/UDP) in your egress rules if pods need to resolve hostnames
- Test your network policies thoroughly to avoid inadvertently blocking required traffic

You can find Network Policy examples in the [`ci/networkpolicy-values.yaml`](https://github.com/Unique-AG/helm-charts/blob/main/charts/backend-service/ci/networkpolicy-values.yaml) (Kubernetes) and [`ci/networkpolicy-cilium-values.yaml`](https://github.com/Unique-AG/helm-charts/blob/main/charts/backend-service/ci/networkpolicy-cilium-values.yaml) (Cilium) files.

### CronJobs
`cronJob` as well as `extraCronJobs` can be used to create cron jobs. These convenience fields are easing the effort to deploy Unique as package. Technically one can also deploy this same chart multiple times but this increases the management complexity on the user side. `cronJob` and `extraCronJobs` allow to deploy multiple cron jobs in a single chart deployment. Note, that they all share the same environment settings for now and that they base on the same image. You should not use this feature if you want to deploy arbitrary or other CronJobs not related to Unique or the current workload/deployment.

⚠️ The syntax between `cronJob` and `extraCronJobs` is different. This is due to the continuous improvement process of Unique where unnecessary complexity has been abstracted for the user. You might still define every property as before, but the chart will default or auto-generate many of the properties that were mandatory in `cronJob`. Unique leaves is open to deprecate `cronJob` in an upcoming major release and generally advises, to directly use `extraCronJobs` for new deployments.

You can find a `extraCronJobs` example in the [`ci/extra-cronjobs-values.yaml`](https://github.com/Unique-AG/helm-charts/blob/main/charts/backend-service/ci/extra-cronjobs-values.yaml) file.

### Audit Volumes
<small>added in `9.0.0`</small>

The chart provides cloud-agnostic audit log storage with blue-green migration support via `auditVolumes`.

#### Structure

```yaml
auditVolumes:
  target: blue  # Volume key or direct path (e.g., "/dev/stdout")
  volumes:
    blue:
      enabled: true
      capacity: 1Ti
      azure:  # Cloud provider detected by presence of this field
        resourceGroup: my-rg
        storageAccount: mystorageaccount
        containerName: audit-logs  # Optional, defaults to release fullname
```

#### Key Concepts

| Field | Description |
|-------|-------------|
| `target` | Where audit logs go. Can be a volume key (`blue`) or direct path (`/dev/stdout`) |
| `volumes` | Map of named volumes. Each creates a PV/PVC pair named `{fullname}-audit-{key}` |
| `azure` | Azure Blob NFS config. Presence of this field enables Azure provider |
| `name` | Optional explicit PV/PVC name for backward compatibility |
| `mountPath` | Optional, defaults to `/audit-{key}` |

#### AUDIT_LOG_DIR Environment Variable

The `AUDIT_LOG_DIR` environment variable is automatically set based on `auditVolumes.target`:

| `target` value | Condition | `AUDIT_LOG_DIR` value |
|----------------|-----------|----------------------|
| Volume key (e.g., `blue`) | Volume `enabled: true` + cloud provider configured | Volume's `mountPath` (or `/audit-{key}` if not set) |
| Volume key (e.g., `blue`) | Volume missing, disabled, or no cloud provider | **Not set** (env var omitted) |
| Direct path (e.g., `/dev/stdout`) | Always | The path directly (e.g., `/dev/stdout`) |
| Not specified | Always | **Not set** (env var omitted) |

> ⚠️ A volume requires both `enabled: true` **and** a cloud provider configuration (e.g., `azure`) to be created. Without a cloud provider, no PV/PVC is created and `AUDIT_LOG_DIR` is not set.

#### Blue-Green Migration

The blue-green pattern allows zero-downtime migration between storage accounts:

1. **Add new volume** under a new key (e.g., `green`)
2. **Deploy** - both volumes will be mounted at different paths
3. **Migrate data** from old to new mount path
4. **Switch target** to new volume key
5. **Disable old volume** (`enabled: false`)

```yaml
auditVolumes:
  target: green  # Switch to new volume
  volumes:
    blue:
      enabled: true  # Keep mounted during migration
      name: my-release-audit  # Existing PV name
      azure:
        resourceGroup: old-rg
        storageAccount: oldaccount
    green:
      enabled: true
      azure:
        resourceGroup: new-rg
        storageAccount: newaccount
```

#### Stdout Logging

For containerized logging without persistent storage:

```yaml
auditVolumes:
  target: /dev/stdout  # AUDIT_LOG_DIR="/dev/stdout", no PV/PVC created
```

## JSON Schema

The chart provides a JSON schema for validating `values.yaml` files. This schema helps with:
- Validating values against the expected structure
- Providing IDE hints and autocompletion when editing values.yaml files
- Documenting the accepted values and their types

The schema is available in the `values.schema.json` file in the chart.

## Azure Key Vault Access

The chart natively supports VM Managed Identity for Azure Key Vault access via the Secrets Store CSI Driver via the `secretProvider` values object. Supported modes:
- [System-assigned Managed Identity](https://azure.github.io/secrets-store-csi-driver-provider-azure/docs/configurations/identity-access-modes/system-assigned-msi-mode/)
- [User-assigned Managed Identity](https://azure.github.io/secrets-store-csi-driver-provider-azure/docs/configurations/identity-access-modes/user-assigned-msi-mode/)

Workload Identity support might be added in the future. If you need it sooner, [open an issue](https://github.com/Unique-AG/helm-charts/issues).

Other authentication methods (service principals, pod identity) are not supported due to the variety of Azure Key Vault access patterns, security policies, and governance requirements across different organizations. The chart provides a stable foundation; extend it using `extraObjects` for custom authentication setups.

## Upgrade Guides

### ~> `9.0.0`

**Breaking change:** The `auditVolume` (singular) configuration has been replaced with `auditVolumes` (plural) - a cloud-agnostic, blue-green migration-ready structure.

**What changed:**
- Removed: `.Values.auditVolume` (singular)
- Added: `.Values.auditVolumes` with `target` and `volumes` structure
- Added: `AUDIT_LOG_DIR` environment variable automatically set based on `target`
- Added: Cloud provider detected by presence of `azure` field (no separate `flavor` needed)
- Added: Explicit `name` field for backward-compatible PV/PVC naming

See [Audit Volumes](#audit-volumes) for full documentation.

**Migration:**

**From (old):**
```yaml
auditVolume:
  enabled: true
  mountPath: /audit
  capacity: 1Ti
  attributes:
    resourceGroup: my-resource-group
    storageAccount: mystorageaccount
    containerName: audit-logs
```

**To (new):**
```yaml
auditVolumes:
  target: blue  # Sets AUDIT_LOG_DIR env var
  volumes:
    blue:
      enabled: true
      # name: my-release-audit  # Optional: use existing PV name for zero-downtime migration
      capacity: 1Ti
      azure:  # Cloud provider detected by presence of this field
        resourceGroup: my-resource-group
        storageAccount: mystorageaccount
        containerName: audit-logs
```

### ~> `8.0.0`

**Breaking change:** Azure Workload Identity moved from `.Values.serviceAccount.workloadIdentity` to `.Values.workloadIdentity.azure`.

**From:**
```yaml
serviceAccount:
  enabled: true
  workloadIdentity:
    enabled: true
    clientId: "00000000-0000-0000-0000-000000000000"
```

**To:**
```yaml
workloadIdentity:
  azure:
    enabled: true
    clientId: "00000000-0000-0000-0000-000000000000"
```

### ~> `7.0.0`

**Breaking changes:** Pod identity and external-secrets support have been removed.

**Who is affected:**
Most users are not affected, [AKS ~1.30](https://learn.microsoft.com/en-us/azure/aks/supported-kubernetes-versions?tabs=azure-cli#kubernetes-130) was the last version with native pod identity support, and external-secrets usage was only Unique-internal but removed.

**What was changed:**
- Pod identity annotations were removed (`aadPodIdBinding`) as well as its use in the `SecretProviderClass`
- The `useVMManagedIdentity` parameter (now always `"true"` internally)
- The `externalSecrets` parameter and template

**Migration:**

If you were using **pod identity**, migrate to `secretProvider` with VM Managed Identity:

```yaml
secretProvider:
  tenantId: your-tenant-id
  userAssignedIdentityID: your-client-id
  vaults:
    your-vault-name:
      ENV_VAR_NAME: secret-name
```

If you were using **external-secrets**, either:
- Use `extraObjects` to include your existing ExternalSecret manifests
- Manage secrets outside the chart using your existing external-secrets operator setup

### ~> `6.0.0`

#### KEDA
The KEDA configuration has been restructured to use a map instead of a list for better overlay support. This allows users to overlay specific triggers without having to redefine the entire list.

**Migration Steps:**

**From (5.x):**
```yaml
keda:
  enabled: true
  scalers:
    - type: rabbitmq
      metadata:
        protocol: amqp
        queueName: testqueue
        mode: QueueLength
        value: "20"
      authenticationRef:
        name: keda-trigger-auth-rabbitmq-conn
    - type: cron
      metadata:
        timezone: Europe/Zurich
        start: 0 6 * * *
        end: 0 20 * * *
        desiredReplicas: "10"
```

**To (6.x):**
```yaml
keda:
  enabled: true
  triggers:
    rabbitmq-trigger:  # arbitrary key name for overlay purposes
      type: rabbitmq
      metadata:
        protocol: amqp
        queueName: testqueue
        mode: QueueLength
        value: "20"
      authenticationRef:
        name: keda-trigger-auth-rabbitmq-conn
    cron-trigger:  # arbitrary key name for overlay purposes
      type: cron
      metadata:
        timezone: Europe/Zurich
        start: 0 6 * * *
        end: 0 20 * * *
        desiredReplicas: "10"
```

**Benefits:**
- Easier to overlay specific triggers without redefining the entire configuration
- Better support for GitOps workflows where different environments may need different triggers
- The map keys are arbitrary and only used for identification during overlays


#### Rationale on circling back to `~4` version like support

Version `~5` removed the previous ScaledObject implementation in favor of a CRD-compliant version that aligns with native KEDA scaler syntax.

Our clients and internal deployments rely heavily on GitOps workflows with ArgoCD. Value overlay capabilities are essential for these use cases. Based on this feedback, we addressed the list structure limitations in this major release.

#### External Secrets
**Note:** External Secrets support has been completely removed in version 7.0.0. See the [7.0.0 upgrade guide](#-700) for migration instructions.

### ~> `5.0.0`

- switch to `.Values.keda` for autoscaling instead of `.Values.eventBasedAutoscaling`. See values file for an example.
- if you don't want to deploy a default route, set `routes.paths.default.enabled` to `false`
- ⚠️ tyk resources are no longer deployed. Please move to Kong.

Migrating alerts:

The chart has introduced a new structured approach to alerts that replaces the previous custom `prometheus.rules` configuration. This new system provides predefined alert groups with sensible defaults while still allowing customization.

#### Key Changes:

1. **Structured Alert Groups**: Instead of manually defining each alert rule, you can now enable predefined alert groups:
   - `defaultAlerts.argocd` - ArgoCD application health and condition alerts
   - `defaultAlerts.kubernetesApplication` - Pod, deployment, and workload health alerts
   - `defaultAlerts.kubernetesResources` - CPU, memory, and storage resource alerts

2. **Custom Alerts**: Use `additionalAlerts` for completely custom alert rules with full control over the alert structure.

#### Migration Steps:

**From:**
```yaml
prometheus:
  enabled: true
  team: backend-team
  rules:
    QNodeChat5xx:
      expression: >
        sum(
          max_over_time(
            nestjs_http_server_responses_total{status=~"^5..$", app="node_chat"}[1m]
          )
          or
          vector(0)
        )
          by (app, path, method, status)
        -
        sum(
          max_over_time(
            nestjs_http_server_responses_total{status=~"^5..$", app="node_chat"}[1m]
            offset 1m
          )
          or vector(0)
        )
          by (app, path, method, status)
        > 0
      for: 0m
      severity: critical
      labels:
        app: app
```

**To:**
```yaml
prometheus:
  enabled: true
  # Enable predefined alert groups
  defaultAlerts:
    # Global labels applied to all alerts
    additionalLabels:
      team: backend-team
      app: app

  # Custom alerts for application-specific monitoring
  additionalAlerts:
    QNodeChat5xx:
      alert: QNodeChat5xx
      expr: >
        sum(
          max_over_time(
            nestjs_http_server_responses_total{status=~"^5..$", app="node_chat"}[1m]
          )
          or
          vector(0)
        )
          by (app, path, method, status)
        -
        sum(
          max_over_time(
            nestjs_http_server_responses_total{status=~"^5..$", app="node_chat"}[1m]
            offset 1m
          )
          or vector(0)
        )
          by (app, path, method, status)
        > 0
      for: 0m
      labels:
        severity: critical
        alertGroup: application
      annotations:
        summary: "High 5xx error rate detected"
        description: "Application {{ `{{ $labels.app }}` }} is experiencing 5xx errors"
```

### ~> `3.0.0`

- `routes` uses `camelCase` for its keys, replace all `lower_snake_case` keys with `camelCase`
- `routes.paths.up` is renamed to `routes.paths.probe`, update your `routes` accordingly

### ~> `2.0.0`

- `httproute` is converted into a dictionary and moves into `extraRoutes`
- `ingress` has been removed. Unique Services require an upstream gateway that authenticates requests and populates the headers with matching information from the JWT token.


## Development

If you want to locally template your chart, you must enable this via the `.Capabilities.APIVersions` [built-in object](https://helm.sh/docs/chart_template_guide/builtin_objects/)
```
helm template some-app oci://ghcr.io/unique-ag/helm-charts/backend-service --api-versions gateway.networking.k8s.io/v1 --values your-own-values.yaml
```

{{ template "helm-docs.versionFooter" . }}
